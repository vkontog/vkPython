{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "qualified-english",
   "metadata": {},
   "source": [
    "<b>Import necessery libraries</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "casual-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "advised-contact",
   "metadata": {
    "gather": {
     "logged": 1641849265971
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.data import DataType\n",
    "from spacy.cli.download import download as spacy_download\n",
    "import os \n",
    "from os.path import join as osjoin\n",
    "import xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "9163eae5",
   "metadata": {
    "gather": {
     "logged": 1641849266347
    }
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-franchise",
   "metadata": {},
   "source": [
    "<b>Select the default workspace & datastore</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "lesbian-teddy",
   "metadata": {
    "gather": {
     "logged": 1641849267303
    }
   },
   "outputs": [],
   "source": [
    "# azureml-core of version 1.0.72 or higher is required\n",
    "# azureml-dataprep[pandas] of version 1.1.34 or higher is required\n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "subscription_id = '6ed9d167-b2e6-41b8-9500-35e6df64d9dc'\n",
    "resource_group = 'MLRG'\n",
    "workspace_name = 'erbbimlws'\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "\n",
    "datastore = workspace.get_default_datastore()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-timing",
   "metadata": {},
   "source": [
    "<b>Loading the Greek language tools</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "clear-segment",
   "metadata": {
    "gather": {
     "logged": 1641849273136
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('el_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "spacy_download('el_core_news_sm')\n",
    "nlp =spacy.load('el_core_news_sm', disable=['tagger', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-alarm",
   "metadata": {},
   "source": [
    "<b>Parameter definitions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "documentary-cleaners",
   "metadata": {
    "gather": {
     "logged": 1641849273586
    }
   },
   "outputs": [],
   "source": [
    "#minimum number of tokens in the texts\n",
    "minCount = 10\n",
    "#ngrams parameters\n",
    "mindf,minngram,maxngram = 30,2,3\n",
    "#keep empty tokens\n",
    "deleteEmptyTokens = True\n",
    "#dataset name to be analyzed\n",
    "datasetName = 'Txns_NLP_202011_part2'\n",
    "#export filename\n",
    "fileName = 'Txns_NLP_202011_part2_exp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-basics",
   "metadata": {},
   "source": [
    "<b>Regular expressions definitions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "greenhouse-chicago",
   "metadata": {
    "gather": {
     "logged": 1641849273985
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "p1 = re.compile('δεν απαντ.{1,3}\\s{0,1}',re.IGNORECASE)\n",
    "p2 = re.compile('\\sδα\\s',re.IGNORECASE)\n",
    "p3 = re.compile('δε.{0,1}\\s.{0,3}\\s{0,1}βρ.{1,2}κ.\\s{0,1}',re.IGNORECASE)\n",
    "p4 = re.compile('[^\\d]?\\d{10}')\n",
    "p5 = re.compile('[^\\d]?\\d{18}|[^\\d]\\d{20}')\n",
    "p6 = re.compile('δε[ ν]{0,1} (επιθυμ[α-ω]{2,4}?|[ήη]θ[εέ]λ[α-ω]{1,3}?|θελ[α-ω]{1,4}|.{0,20}ενδιαφ[εέ]ρ[α-ω]{2,4})',re.IGNORECASE)\n",
    "p7 = re.compile('δε[ ν]{0,1} (μπορ[α-ω]{2,5}|.εχει)',re.IGNORECASE)\n",
    "p8 = re.compile('(δεν|μη).*διαθεσιμ[οη]ς{0,1}?',re.IGNORECASE)\n",
    "p9 = re.compile('(δεν|μη)+.*εφικτη?',re.IGNORECASE)\n",
    "p10 = re.compile('δε[ ν]{0,1}.{0,20}θετικ[οόήη]ς{0,1}',re.IGNORECASE)\n",
    "\n",
    "#pinakides\n",
    "#p11 = re.compile('\\s([a-zA-Zα-ωΑ-Ω]{3}\\s*?[0-9]{3,4})\\s',re.IGNORECASE)\n",
    "p11 = re.compile('\\s[a-zA-Zα-ωΑ-Ω]{3}[0-9]{3,4}\\s',re.IGNORECASE)\n",
    "\n",
    "#enoikia\n",
    "p12=re.compile('εν(.{1,2}κ.{1,3})',re.IGNORECASE)\n",
    "p13=re.compile('en(.{1,2}k.{1,3})',re.IGNORECASE)\n",
    "\n",
    "p14 = re.compile('(φυσιοθεραπ|ορθοπαιδικος|ΩΡΛ|ΩΝΑΣΕΙΟ|ψυχολογος|ψυχοθεραπεια|ψυχιατρος)\\S*' ,re.IGNORECASE)\n",
    "p15 = re.compile('(EX/SH|Εξοφληση|ΕΞΟΦΛΗΣΗ|eksoflisi|ΕΞΩΦΛΗΣΗ|Εξόφλησ)\\S*' ,re.IGNORECASE)\n",
    "p16 = re.compile('(ASFALIA|asfaleia|ασφάλ|asfalistra)\\S*' ,re.IGNORECASE)\n",
    "p17 = re.compile('(ΤΙΜΟΛΟΓΙΟ|ΤΙΜΟΛ|ΤΙΜΟΛΟΓΊΟΥ|TIM|ΠΡΟΦΟΡΜΑ)\\S*' ,re.IGNORECASE)\n",
    "p18 = re.compile('(ΝΕΡΟΥ|ΝΕΡΟ|ευδαπ|ΔΕΗ|dei|ρευμα|nero|nerou|eydap)\\S*' ,re.IGNORECASE)\n",
    "p19 = re.compile('(ΜΙΣΘΟΔΟΣΙΑ|Αδεια|ΑΠΟΔΟΧΕΣ|μηνιατικο|μισθοδοσία|ΕΞΟΦΛ ΜΙΣΘ|ΜΙΣΘΟΣ|ΜΙΣΘΟΔΩΣΙΑ|ΜΙΣΘ/ΣΙΑ|ΔΩΡΟ ΧΡΙΣΟΥΓΕΝΩΝ|ΔΩΡΟ ΧΡΙΣΤ|ΟΔΟΙΠΟΡΙΚΑ)\\S*' ,re.IGNORECASE)\n",
    "p20 = re.compile('(ΕΝΦΙΑ|enfia|ΤΑΠ ΑΚΙΝΗΤΟΥ|ΚΤΗΜΑΤΟΛΟΓΙΟ)\\S*' ,re.IGNORECASE)\n",
    "p21 = re.compile('(ΕΦΚΑ|efka|foros|φορος|φορος εισοδηματος|ΦΠΑ)\\S*' ,re.IGNORECASE)\n",
    "p22 = re.compile('(KOINOXΡHΣTA|ΚΟΙΝ/ΣΤΑ|Συντηρηση Ανελκυστηρα|ΚΟΙΝ|Κοινόχροιστα|KOINOXRISTA)\\S*' ,re.IGNORECASE)\n",
    "p23 = re.compile('(να ζησετε|δωρο γαμου|na zisete|gamou|gamos|γαμος)\\S*' ,re.IGNORECASE)\n",
    "p24 = re.compile('(να σας ζησει|na sas zisei|βαφτιση|vaftisi|baftisi)\\S*' ,re.IGNORECASE)\n",
    "p25 = re.compile('(δοση σπίτι|Δάνειο σπίτι|δόση στεγαστικού δανείου|ΣΤΕΓΑΣΤΙΚΟ  ΔΑΝΕΙΟ|Πληρωμη στεγαστικου)\\S*' ,re.IGNORECASE)\n",
    "p26 = re.compile('(μτφ|metafora|μεταφορα|mtf)\\S*' ,re.IGNORECASE)\n",
    "p27 = re.compile('(ORDER|παραγγελίας|ΠΑΡΑΓΓΕΛΙΑ|αρ.παρ.|PARAGGELIAS|PARAGGELIA|αρ.παρ|orders)\\S*' ,re.IGNORECASE)\n",
    "p28 = re.compile('(xreos|ΔΑΝΕΙΚΑ|ΕΝΑΝΤΙ ΛΟΓΑΡΙΑΣΜΟΥ|εξοδα|ΕΞΟΔ.|expenses|οφειλη|χρεος)\\S*' ,re.IGNORECASE)\n",
    "p29 = re.compile('(κτθ|καταθεση|katathesi|ktth)\\S*' ,re.IGNORECASE)\n",
    "p30 = re.compile('(ΠΡΟΚΑΤΑ|ΠΡΟΚΑΤΑΒΟΛΉ|ΠΡΟΚΑΤΑΒ|prokatavoli|prokataboli|prokatav)\\S*' ,re.IGNORECASE)\n",
    "p31 = re.compile('(DIATROFI|ΔΙΑΤΡΟΦΗ|ΔΙΑΤΡΟΦΙ)\\S*' ,re.IGNORECASE)\n",
    "p32 = re.compile('(δοση αυτοκινητου|δανειο αυτοκινητου|daneiou autokinitou|daneio autokinitou|dosi autokonitou|daneiou autokinito)\\S*' ,re.IGNORECASE)\n",
    "p33 = re.compile('(ΠΛΗΡΩΜΗ ΔΑΝΕΙΟΥ|dosi daniou|εξοικονομω|Πληρωμη δοσης|δοση δανειου|ΔΟΣΗ ΔΑΝΕΙΟΥ|DOSΗ ΔΑΝΕΙΟΥ|ΔΌΣΗΔΑΝΕΊΟΥ|ΠΛΥΡΟΜΗ.ΔΑΝΕΙΟΥ)\\S*' ,re.IGNORECASE)\n",
    "p34 = re.compile('(Πληρωμή καρτας|ΠΛΗΡΩΜΉ ΠΙΣΤΩΤΙΚΗΣ)\\S*' ,re.IGNORECASE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-bobby",
   "metadata": {},
   "source": [
    "<b>Functions definitions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "permanent-tolerance",
   "metadata": {
    "gather": {
     "logged": 1641849274334
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def loadStopWords(ws):\n",
    "    #A dataset containing the Greek stop words has been created\n",
    "    #the function loads this dataset as a dataframe\n",
    "    dataset = Dataset.get_by_name(ws, name='stopWords_gr')\n",
    "    sw = set(dataset.to_pandas_dataframe())\n",
    "    return sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "documentary-nelson",
   "metadata": {
    "gather": {
     "logged": 1641849274639
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def replaceTerm(text):\n",
    "    #This function uses the above defined regular expressions to replace text\n",
    "    #The order of the rules is importand\n",
    "    #Compinations of two or more words, are concatenated, in order to be considered as a single token\n",
    "    \n",
    "    #text = str(text)\n",
    "    \n",
    "    text = p5.sub(' λογαριασμός ',text)\n",
    "    text = p4.sub(' τηλεφωνο ',text)\n",
    "    text = p6.sub(' δενθελειδενενδιαφερεται ',text)\n",
    "    text = p10.sub(' δενθελειδενενδιαφερεται ',text)\n",
    "    text = p7.sub(' δενεχειδενμπορει ',text)\n",
    "    text = p8.sub(' δενειναιδιαθεσιμος ',text)\n",
    "    text = p9.sub(' ανεφικτη ',text)\n",
    "    text = text.replace('-banking','banking')\n",
    "    text = text.replace('v banking','vbanking')\n",
    "    text = text.replace('e banking','ebanking')\n",
    "    text = text.replace('follow up','followup')\n",
    "    text = text.replace('fup','followup')\n",
    "    text = text.replace('f/up','followup')\n",
    "    text = text.replace('πυρ/ριο','πυρασφαλιστηριο')\n",
    "    text = text.replace('safe drive','safedrive')\n",
    "    text = text.replace('safe pocket','safepocket')\n",
    "    text = text.replace('alphabank','alpha')\n",
    "    text = text.replace('sweet home smart','sweethomesmart')\n",
    "    text = text.replace('sweet home','sweethome')\n",
    "    text = text.replace('eξασφαλιζω','εξασφαλιζω')\n",
    "    text = text.replace('credit card','creditcard')\n",
    "    text = text.replace('debit card','debitcard')\n",
    "    text = text.replace('life cycle','lifecycle')\n",
    "    text = text.replace('π/κ','πκ')\n",
    "    text = text.replace('td','πκ')\n",
    "    text = text.replace('α/κ','ακ')\n",
    "    text = text.replace('δ/α','δεναπαντα ')\n",
    "    text = text.replace('εκτος αττικης','εκτοςαττικης ')\n",
    "    text = text.replace('PaF online payments','πληρωμήPaF')\n",
    "    text = text.replace('ΚΤΘ APS', 'κατάθεσηAPS')\n",
    "    text = text.replace('ΝΟΜΟΣ 3869', 'ΝΟΜΟΣ3869')\n",
    "    text = text.replace('ΜΤΦ ΜΕΣΩ WEB','ΣυστημικόΜΤΦebanking')\n",
    "    text = text.replace('VISA GOLD', 'VISAGOLD')\n",
    "    text = text.replace('Ασφαλεια αυτο/του','ασφαλειααυτοκινήτου')\n",
    "    text = text.replace('ασφαλεια αυτοκινητου','ασφαλειααυτοκινήτου')\n",
    "    text = text.replace('ασφαλειων αυτ/των','ασφαλειααυτοκινήτου')\n",
    "    text = text.replace('ΑΣΦΆΛΕΙΑ ΣΠΙΤΙΟΎ','ασφαλειασπιτιου')\n",
    "    text = text.replace('POI',' εισερχομενοεμβασμα ')\n",
    "    text = text.replace('POO',' εξερχομενοεμβασμα ')\n",
    "    \n",
    "    \n",
    "    #τδ\n",
    "    text = p1.sub(' δεναπαντα ',text)\n",
    "    text = p2.sub(' δεναπαντα ',text)\n",
    "    text = p3.sub(' δεντονβρηκα ',text)\n",
    "    text = p17.sub(' τιμολογια ',text)\n",
    "    text = p11.sub(' αριθμοςκυκλοφοριας ', text)\n",
    "    text = p12.sub(' ενοικιο ',text)\n",
    "    text = p13.sub(' ενοικιο ',text)\n",
    "    text = p14.sub(' γιατροί ',text)\n",
    "    text = p15.sub(' εξόφληση ',text)\n",
    "    text = p16.sub(' ασφάλειες ',text)\n",
    "    \n",
    "    text = p18.sub(' ΔΕΚΟ ',text)\n",
    "    text = p19.sub(' μισθοδοσιες ', text)\n",
    "    text = p20.sub(' εφοριαακινητων ', text)\n",
    "    text = p21.sub(' εφορια ', text)\n",
    "    text = p22.sub(' κοινοχρηστα ', text)\n",
    "    text = p23.sub(' συναλλαγεςγαμου ', text)\n",
    "    text = p24.sub(' συναλλαγεςγεννησηςβαφτισης ', text)\n",
    "    text = p25.sub(' στεγαστικοδανειο ', text)\n",
    "    text = p26.sub(' μεταφορες ', text)\n",
    "    text = p27.sub(' παραγγελίες ', text)\n",
    "    text = p28.sub(' οφειλες ', text)\n",
    "    text = p29.sub(' καταθεσεις ', text)\n",
    "    text = p30.sub(' προκαταβολες ', text)\n",
    "    text = p31.sub(' διατροφη ', text)\n",
    "    text = p32.sub(' δανειοαυτοκινητου ', text)\n",
    "    text = p33.sub(' πληρωμηδανειου ', text)\n",
    "    text = p34.sub(' πληρωμηπιστωτικης ', text)\n",
    "    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "static-crossing",
   "metadata": {
    "gather": {
     "logged": 1641849275080
    }
   },
   "outputs": [],
   "source": [
    "def remove_ton(text):\n",
    "    #removes punctuation, αφαιρεί τους τόνους\n",
    "    diction = {'ά':'α','έ':'ε','ί':'ι','ό':'ο','ώ':'ω','ύ':'υ'}\n",
    "    for key in diction.keys():\n",
    "        text = text.replace(key, diction[key])\n",
    "    return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "painful-dairy",
   "metadata": {
    "gather": {
     "logged": 1641849275363
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def clean_text(text,sw):\n",
    "    #This function performs text cleansing and returns the clean and lemmatized version of the original text\n",
    "    #conver to lower text \n",
    "    text = str(text).lower()\n",
    "    \n",
    "   # remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "\n",
    "    # αφαιρούνται οι τόνοι\n",
    "    text = [remove_ton(x) for x in text]\n",
    "    \n",
    "    # remove stop words\n",
    "    text = [x for x in text if x not in sw]\n",
    "    \n",
    "    #replacements either by rules or regular expressions\n",
    "    \n",
    "    \n",
    "    #remove quotes\n",
    "    text = [x.replace('quot;','').replace('&quot','') for x in text if x not in {'quot','amp'}]\n",
    "    \n",
    "    # remove words that contain numbers\n",
    "    #text = [word for word in text if not any(c.isdigit() for c in word)] #addition to return even empty tokens\n",
    "    \n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0] #addition to return even empty tokens\n",
    "    \n",
    "    # remove amp & quot\n",
    "    text = [x for x in text if x not in ['quot','amp']]\n",
    "    \n",
    "    # remove words with only one letter\n",
    "    text = \" \".join([t for t in text if len(t) > 0]) #addition to return even empty tokens\n",
    "    \n",
    "    text = replaceTerm(text)\n",
    "    # lemmatize text\n",
    "    text = \" \".join([t.lemma_ for t in nlp(text, disable=['tagger', 'parser', 'ner','tok2vec', 'morphologizer', 'parser', 'senter', 'attribute_ruler',  'ner'])])\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "spoken-novel",
   "metadata": {
    "gather": {
     "logged": 1641849275657
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def load_correctDict(ws):\n",
    "    #it creates a dictionary out of a dataset that containes pairs of (original term, corrected term)    \n",
    "    dataset = Dataset.get_by_name(ws, name='correct_Tokens')    \n",
    "    corDict = dict(dataset.to_pandas_dataframe().to_dict(\"split\")['data'])\n",
    "    return corDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "passing-suite",
   "metadata": {
    "gather": {
     "logged": 1641849275995
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def correct(x,corDict):\n",
    "    #uses the dictionary to correct the terms\n",
    "    if x in corDict.keys():\n",
    "        y = corDict[x]\n",
    "    else:\n",
    "        y = x\n",
    "    return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "global-chess",
   "metadata": {
    "gather": {
     "logged": 1641849276257
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_ngrams(idf,mindf,minngram,maxngram):\n",
    "    #this function returns the bi-grams and tri-grams\n",
    "    tfidf = TfidfVectorizer(min_df = mindf,ngram_range = (minngram,maxngram))\n",
    "    tfidf_result = tfidf.fit_transform(idf['tokenized']).toarray()\n",
    "    tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())\n",
    "    tfidf_df.columns = [str(x) for x in tfidf_df.columns]\n",
    "    df_i = pd.concat([df[['CON_ROW_ID']],tfidf_df],axis=1).melt(id_vars=['CON_ROW_ID'],value_vars = tfidf_df.columns).dropna()\n",
    "    df_i = df_i[df_i['value']>0]\n",
    "    return df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "environmental-pointer",
   "metadata": {
    "gather": {
     "logged": 1641849276523
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def cleanComments(df,sw):\n",
    "    #applies the clean text function to all texts contained in the dataset\n",
    "    df = df[['CON_ROW_ID','CON_COMMENTS']]\n",
    "    df['tokenized'] = df['CON_COMMENTS'].apply(clean_text,args = (sw))\n",
    "    df = df.fillna('N/A')\n",
    "    df['variable'] = df['tokenized'].str.split()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "improving-smoke",
   "metadata": {
    "gather": {
     "logged": 1641849276881
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def getTokens(df,sw):\n",
    "    #The variable columns is a list. The explode method \"unpivots this list\"\n",
    "    df = cleanComments(df,sw)\n",
    "    df_f = df.explode('variable')[['CON_ROW_ID','variable']]\n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "personal-hands",
   "metadata": {
    "gather": {
     "logged": 1641849277182
    }
   },
   "outputs": [],
   "source": [
    "def getTokencount(df_f,minCount):\n",
    "    #calculate the number of occurances (counts) of each token\n",
    "    #tokens with count less than mincount are set to blank\n",
    "    tokenCount = df_f['variable'].value_counts().to_dict()\n",
    "    \n",
    "    df_f['value'] = df_f['variable'].map(tokenCount)\n",
    "   \n",
    "    df_f.loc[(df_f['value']<minCount), 'variable'] = ' ' #addition to return even empty tokens\n",
    "    \n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "bizarre-finger",
   "metadata": {
    "gather": {
     "logged": 1641850635100
    }
   },
   "outputs": [],
   "source": [
    "def performNLP(workspace,minCount,mindf,minngram,maxngram,deleteEmptyTokens,df):\n",
    "    sw = loadStopWords(workspace)\n",
    "    \n",
    "    df = cleanComments(df,sw)\n",
    "    \n",
    "    df_f = getTokens(df,sw)\n",
    "\n",
    "    df_f.count()\n",
    "    \n",
    "    df_f = df_f.fillna(' ')\n",
    "    \n",
    "    df_f = getTokencount(df_f,minCount)\n",
    "    \n",
    "    #try:\n",
    "    #    df_f = df_f.append(get_ngrams(df,mindf,minngram,maxngram ))\n",
    "    #except:\n",
    "    #    print('no bigramms or trigramms were added')\n",
    "    \n",
    "    #corDict = load_correctDict(workspace)     \n",
    "    \n",
    "    #df_f['token'] = df_f['variable'].apply(lambda x : correct(x,corDict))\n",
    "    \n",
    "    #df_f.loc[(df_f['token'].str.len() <2), 'token'] = ' ' #single character tokens are set to blank\n",
    "    \n",
    "    df_f['token'] = df_f['variable']\n",
    "    \n",
    "    df_f = df_f.sort_values(['CON_ROW_ID','token'])\n",
    "    \n",
    "    if deleteEmptyTokens:\n",
    "        df_f = df_f[df_f['token'] != ' ']\n",
    "    \n",
    "    df_f = df_f[['CON_ROW_ID','token']].drop_duplicates()\n",
    "    \n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "synthetic-density",
   "metadata": {
    "gather": {
     "logged": 1641849277867
    }
   },
   "outputs": [],
   "source": [
    "def loadTexts(workspace,datasetName):\n",
    "    #loads the texts to be analyzed\n",
    "    dataset = Dataset.get_by_name(workspace, name=datasetName)\n",
    "    df = dataset.to_pandas_dataframe()\n",
    "    df= df[['CON_ROW_ID','CON_COMMENTS']]\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "intimate-architect",
   "metadata": {
    "gather": {
     "logged": 1641849278163
    }
   },
   "outputs": [],
   "source": [
    "def exportResults(workspace,datastore,fileName,df_f):\n",
    "    df_f.to_csv(fileName+'.txt',sep =',',line_terminator='\\r\\n',index = False)\n",
    "    fil = [os.getcwd()+'/'+ fileName+'.txt']\n",
    "    datastore.upload_files(fil, target_path='UI/NLP', overwrite=True, show_progress=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-channels",
   "metadata": {},
   "source": [
    "<b>The commended-out code is for debuging purposes</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "choice-order",
   "metadata": {
    "gather": {
     "logged": 1641849278513
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#txt = 'H eurobank είναι καλύτερη τράπεζα στον κόσμο'\n",
    "#com = {'CON_ROW_ID':[1],'CON_COMMENTS':[txt]}\n",
    "#df = pd.DataFrame(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "nasty-farmer",
   "metadata": {
    "gather": {
     "logged": 1641849279404
    }
   },
   "outputs": [],
   "source": [
    "df = loadTexts(workspace,datasetName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "f83ed741-f1e0-461b-a11b-c4f5fb26ee16",
   "metadata": {
    "gather": {
     "logged": 1641851325524
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#df = df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "0cd6dc54-39db-47b6-9320-35cdea7b10cc",
   "metadata": {
    "gather": {
     "logged": 1641851333697
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CON_ROW_ID</th>\n",
       "      <th>CON_COMMENTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2673060</td>\n",
       "      <td>19662205 ΤΑΜΑΝΑΚΗ ΧΡΥΣΟΥΛΑ    &amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6182214</td>\n",
       "      <td>1966346                       &amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6053283</td>\n",
       "      <td>1966346                       &amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6573619</td>\n",
       "      <td>19664841 ΔΑΣΚΑΛΑΚΗ ΕΥΑΓ.ΜΑΡΚΟΣ&amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6573744</td>\n",
       "      <td>19664841 ΔΑΣΚΑΛΑΚΗ ΕΥΑΓ.ΜΑΡΚΟΣ&amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1576787</td>\n",
       "      <td>1o GYMNASIO RAFINAS 256/06-11-&amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1170116</td>\n",
       "      <td>1o GYMNASIO RAFINAS 256/06-11-&amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>4538035</td>\n",
       "      <td>1o GYMNASIO RAFINAS 531/31-10-&amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>3926532</td>\n",
       "      <td>1o GYMNASIO RAFINAS 531/31-10-&amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>12367291</td>\n",
       "      <td>1o GYMNASIO RAFINAS A870/4-11-&amp;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CON_ROW_ID                     CON_COMMENTS\n",
       "0       2673060  19662205 ΤΑΜΑΝΑΚΗ ΧΡΥΣΟΥΛΑ    &\n",
       "1       6182214  1966346                       &\n",
       "2       6053283  1966346                       &\n",
       "3       6573619  19664841 ΔΑΣΚΑΛΑΚΗ ΕΥΑΓ.ΜΑΡΚΟΣ&\n",
       "4       6573744  19664841 ΔΑΣΚΑΛΑΚΗ ΕΥΑΓ.ΜΑΡΚΟΣ&\n",
       "..          ...                              ...\n",
       "995     1576787  1o GYMNASIO RAFINAS 256/06-11-&\n",
       "996     1170116  1o GYMNASIO RAFINAS 256/06-11-&\n",
       "997     4538035  1o GYMNASIO RAFINAS 531/31-10-&\n",
       "998     3926532  1o GYMNASIO RAFINAS 531/31-10-&\n",
       "999    12367291  1o GYMNASIO RAFINAS A870/4-11-&\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "hungarian-graphic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "df_f = performNLP(workspace, minCount, mindf, minngram, maxngram, deleteEmptyTokens, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "neural-lloyd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CON_ROW_ID</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67850</th>\n",
       "      <td>5488</td>\n",
       "      <td>2o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67850</th>\n",
       "      <td>5488</td>\n",
       "      <td>noembrioy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8113</th>\n",
       "      <td>5505</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8113</th>\n",
       "      <td>5505</td>\n",
       "      <td>τηλεφωνο</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8116</th>\n",
       "      <td>5558</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58760</th>\n",
       "      <td>261329</td>\n",
       "      <td>αε</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>261674</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89334</th>\n",
       "      <td>262104</td>\n",
       "      <td>arisagor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89334</th>\n",
       "      <td>262104</td>\n",
       "      <td>stoiximangr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89584</th>\n",
       "      <td>262283</td>\n",
       "      <td>arisagor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CON_ROW_ID        token\n",
       "67850        5488           2o\n",
       "67850        5488    noembrioy\n",
       "8113         5505            5\n",
       "8113         5505     τηλεφωνο\n",
       "8116         5558            6\n",
       "...           ...          ...\n",
       "58760      261329           αε\n",
       "2568       261674            2\n",
       "89334      262104     arisagor\n",
       "89334      262104  stoiximangr\n",
       "89584      262283     arisagor\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "collectible-tulsa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading /mnt/resource/batch/tasks/shared/LS_root/mounts/clusters/vkontogcompute/code/Users/mchouliarea/Txns_NLP_202011_part2_exp.txt\n",
      "Uploaded /mnt/resource/batch/tasks/shared/LS_root/mounts/clusters/vkontogcompute/code/Users/mchouliarea/Txns_NLP_202011_part2_exp.txt, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    }
   ],
   "source": [
    "exportResults(workspace,datastore,fileName,df_f)\n",
    "\n",
    "#run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "apparent-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_f,df, how=\"inner\",on=['CON_ROW_ID']).head(10000000).to_excel('NLP{0}.xlsx'.format(fileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "organized-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_f['token'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "caroline-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['CON_ROW_ID']==5355548]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "persistent-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_f[df_f['token']=='αριθμοςκυκλοφοριας']"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
